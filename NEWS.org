* Version 0.5.2
- Fix incompatibility with older Emacs introduced in Version 0.5.1.
- Add support for Google Cloud Vertex model =text-bison= and variants.
- =llm-ollama= can now be configured with a scheme (http vs https).
* Version 0.5.1
- Implement token counting for Google Cloud Vertex via their API.
- Fix issue with Google Cloud Vertex erroring on multibyte strings.
- Fix issue with small bits of missing text in Open AI and Ollama streaming chat.
* Version 0.5
- Fixes for conversation context storage, requiring clients to handle ongoing conversations slightly differently.
- Fixes for proper sync request http error code handling.
- =llm-ollama= can now be configured with a different hostname.
- Callbacks now always attempts to be in the client's original buffer.
- Add provider =llm-gpt4all=.
* Version 0.4
- Add helper function ~llm-chat-streaming-to-point~.
- Add provider =llm-ollama=.
* Version 0.3
- Streaming support in the API, and for the Open AI and Vertex models.
- Properly encode and decode in utf-8 so double-width or other character sizes don't cause problems.
* Version 0.2.1
- Changes in how we make and listen to requests, in preparation for streaming functionality.
- Fix overzealous change hook creation when using async llm requests.
* Version 0.2
- Remove the dependency on non-GNU request library.
